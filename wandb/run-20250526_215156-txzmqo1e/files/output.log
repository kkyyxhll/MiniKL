开始载入数据集,path:/home/kkyyxhll/Projects/PythonProjects/MiniKL/data/out/data0.jsonl
数据集载入完成,time:6.44217848777771
Traceback (most recent call last):
  File "/home/kkyyxhll/Projects/PythonProjects/MiniKL/train/train_pretrain.py", line 101, in <module>
    model.load_state_dict(torch.load(args.model_save_path))
  File "/home/kkyyxhll/anaconda3/envs/dist_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for MiniKLModel:
	Missing key(s) in state_dict: "embeddings.weight", "layers.0.attention.rope.rope_sin", "layers.0.attention.rope.rope_cos", "layers.0.attention.w_q.weight", "layers.0.attention.w_q.bias", "layers.0.attention.w_k.weight", "layers.0.attention.w_k.bias", "layers.0.attention.w_v.weight", "layers.0.attention.w_v.bias", "layers.0.attention.w_o.weight", "layers.0.attention.w_o.bias", "layers.0.ffn.w_up_a.weight", "layers.0.ffn.w_up_a.bias", "layers.0.ffn.w_up_b.weight", "layers.0.ffn.w_up_b.bias", "layers.0.ffn.w_down.weight", "layers.0.ffn.w_down.bias", "layers.0.norm1.weight", "layers.0.norm1.bias", "layers.0.norm2.weight", "layers.0.norm2.bias", "layers.1.attention.rope.rope_sin", "layers.1.attention.rope.rope_cos", "layers.1.attention.w_q.weight", "layers.1.attention.w_q.bias", "layers.1.attention.w_k.weight", "layers.1.attention.w_k.bias", "layers.1.attention.w_v.weight", "layers.1.attention.w_v.bias", "layers.1.attention.w_o.weight", "layers.1.attention.w_o.bias", "layers.1.ffn.w_up_a.weight", "layers.1.ffn.w_up_a.bias", "layers.1.ffn.w_up_b.weight", "layers.1.ffn.w_up_b.bias", "layers.1.ffn.w_down.weight", "layers.1.ffn.w_down.bias", "layers.1.norm1.weight", "layers.1.norm1.bias", "layers.1.norm2.weight", "layers.1.norm2.bias", "layers.2.attention.rope.rope_sin", "layers.2.attention.rope.rope_cos", "layers.2.attention.w_q.weight", "layers.2.attention.w_q.bias", "layers.2.attention.w_k.weight", "layers.2.attention.w_k.bias", "layers.2.attention.w_v.weight", "layers.2.attention.w_v.bias", "layers.2.attention.w_o.weight", "layers.2.attention.w_o.bias", "layers.2.ffn.w_up_a.weight", "layers.2.ffn.w_up_a.bias", "layers.2.ffn.w_up_b.weight", "layers.2.ffn.w_up_b.bias", "layers.2.ffn.w_down.weight", "layers.2.ffn.w_down.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm2.weight", "layers.2.norm2.bias", "layers.3.attention.rope.rope_sin", "layers.3.attention.rope.rope_cos", "layers.3.attention.w_q.weight", "layers.3.attention.w_q.bias", "layers.3.attention.w_k.weight", "layers.3.attention.w_k.bias", "layers.3.attention.w_v.weight", "layers.3.attention.w_v.bias", "layers.3.attention.w_o.weight", "layers.3.attention.w_o.bias", "layers.3.ffn.w_up_a.weight", "layers.3.ffn.w_up_a.bias", "layers.3.ffn.w_up_b.weight", "layers.3.ffn.w_up_b.bias", "layers.3.ffn.w_down.weight", "layers.3.ffn.w_down.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm2.weight", "layers.3.norm2.bias", "fc.weight", "fc.bias".
	Unexpected key(s) in state_dict: "module.embeddings.weight", "module.layers.0.attention.rope.rope_sin", "module.layers.0.attention.rope.rope_cos", "module.layers.0.attention.w_q.weight", "module.layers.0.attention.w_q.bias", "module.layers.0.attention.w_k.weight", "module.layers.0.attention.w_k.bias", "module.layers.0.attention.w_v.weight", "module.layers.0.attention.w_v.bias", "module.layers.0.attention.w_o.weight", "module.layers.0.attention.w_o.bias", "module.layers.0.ffn.w_up_a.weight", "module.layers.0.ffn.w_up_a.bias", "module.layers.0.ffn.w_up_b.weight", "module.layers.0.ffn.w_up_b.bias", "module.layers.0.ffn.w_down.weight", "module.layers.0.ffn.w_down.bias", "module.layers.0.norm1.weight", "module.layers.0.norm1.bias", "module.layers.0.norm2.weight", "module.layers.0.norm2.bias", "module.layers.1.attention.rope.rope_sin", "module.layers.1.attention.rope.rope_cos", "module.layers.1.attention.w_q.weight", "module.layers.1.attention.w_q.bias", "module.layers.1.attention.w_k.weight", "module.layers.1.attention.w_k.bias", "module.layers.1.attention.w_v.weight", "module.layers.1.attention.w_v.bias", "module.layers.1.attention.w_o.weight", "module.layers.1.attention.w_o.bias", "module.layers.1.ffn.w_up_a.weight", "module.layers.1.ffn.w_up_a.bias", "module.layers.1.ffn.w_up_b.weight", "module.layers.1.ffn.w_up_b.bias", "module.layers.1.ffn.w_down.weight", "module.layers.1.ffn.w_down.bias", "module.layers.1.norm1.weight", "module.layers.1.norm1.bias", "module.layers.1.norm2.weight", "module.layers.1.norm2.bias", "module.layers.2.attention.rope.rope_sin", "module.layers.2.attention.rope.rope_cos", "module.layers.2.attention.w_q.weight", "module.layers.2.attention.w_q.bias", "module.layers.2.attention.w_k.weight", "module.layers.2.attention.w_k.bias", "module.layers.2.attention.w_v.weight", "module.layers.2.attention.w_v.bias", "module.layers.2.attention.w_o.weight", "module.layers.2.attention.w_o.bias", "module.layers.2.ffn.w_up_a.weight", "module.layers.2.ffn.w_up_a.bias", "module.layers.2.ffn.w_up_b.weight", "module.layers.2.ffn.w_up_b.bias", "module.layers.2.ffn.w_down.weight", "module.layers.2.ffn.w_down.bias", "module.layers.2.norm1.weight", "module.layers.2.norm1.bias", "module.layers.2.norm2.weight", "module.layers.2.norm2.bias", "module.layers.3.attention.rope.rope_sin", "module.layers.3.attention.rope.rope_cos", "module.layers.3.attention.w_q.weight", "module.layers.3.attention.w_q.bias", "module.layers.3.attention.w_k.weight", "module.layers.3.attention.w_k.bias", "module.layers.3.attention.w_v.weight", "module.layers.3.attention.w_v.bias", "module.layers.3.attention.w_o.weight", "module.layers.3.attention.w_o.bias", "module.layers.3.ffn.w_up_a.weight", "module.layers.3.ffn.w_up_a.bias", "module.layers.3.ffn.w_up_b.weight", "module.layers.3.ffn.w_up_b.bias", "module.layers.3.ffn.w_down.weight", "module.layers.3.ffn.w_down.bias", "module.layers.3.norm1.weight", "module.layers.3.norm1.bias", "module.layers.3.norm2.weight", "module.layers.3.norm2.bias", "module.fc.weight", "module.fc.bias".
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/kkyyxhll/Projects/PythonProjects/MiniKL/train/train_pretrain.py", line 101, in <module>
[rank0]:     model.load_state_dict(torch.load(args.model_save_path))
[rank0]:   File "/home/kkyyxhll/anaconda3/envs/dist_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: Error(s) in loading state_dict for MiniKLModel:
[rank0]: 	Missing key(s) in state_dict: "embeddings.weight", "layers.0.attention.rope.rope_sin", "layers.0.attention.rope.rope_cos", "layers.0.attention.w_q.weight", "layers.0.attention.w_q.bias", "layers.0.attention.w_k.weight", "layers.0.attention.w_k.bias", "layers.0.attention.w_v.weight", "layers.0.attention.w_v.bias", "layers.0.attention.w_o.weight", "layers.0.attention.w_o.bias", "layers.0.ffn.w_up_a.weight", "layers.0.ffn.w_up_a.bias", "layers.0.ffn.w_up_b.weight", "layers.0.ffn.w_up_b.bias", "layers.0.ffn.w_down.weight", "layers.0.ffn.w_down.bias", "layers.0.norm1.weight", "layers.0.norm1.bias", "layers.0.norm2.weight", "layers.0.norm2.bias", "layers.1.attention.rope.rope_sin", "layers.1.attention.rope.rope_cos", "layers.1.attention.w_q.weight", "layers.1.attention.w_q.bias", "layers.1.attention.w_k.weight", "layers.1.attention.w_k.bias", "layers.1.attention.w_v.weight", "layers.1.attention.w_v.bias", "layers.1.attention.w_o.weight", "layers.1.attention.w_o.bias", "layers.1.ffn.w_up_a.weight", "layers.1.ffn.w_up_a.bias", "layers.1.ffn.w_up_b.weight", "layers.1.ffn.w_up_b.bias", "layers.1.ffn.w_down.weight", "layers.1.ffn.w_down.bias", "layers.1.norm1.weight", "layers.1.norm1.bias", "layers.1.norm2.weight", "layers.1.norm2.bias", "layers.2.attention.rope.rope_sin", "layers.2.attention.rope.rope_cos", "layers.2.attention.w_q.weight", "layers.2.attention.w_q.bias", "layers.2.attention.w_k.weight", "layers.2.attention.w_k.bias", "layers.2.attention.w_v.weight", "layers.2.attention.w_v.bias", "layers.2.attention.w_o.weight", "layers.2.attention.w_o.bias", "layers.2.ffn.w_up_a.weight", "layers.2.ffn.w_up_a.bias", "layers.2.ffn.w_up_b.weight", "layers.2.ffn.w_up_b.bias", "layers.2.ffn.w_down.weight", "layers.2.ffn.w_down.bias", "layers.2.norm1.weight", "layers.2.norm1.bias", "layers.2.norm2.weight", "layers.2.norm2.bias", "layers.3.attention.rope.rope_sin", "layers.3.attention.rope.rope_cos", "layers.3.attention.w_q.weight", "layers.3.attention.w_q.bias", "layers.3.attention.w_k.weight", "layers.3.attention.w_k.bias", "layers.3.attention.w_v.weight", "layers.3.attention.w_v.bias", "layers.3.attention.w_o.weight", "layers.3.attention.w_o.bias", "layers.3.ffn.w_up_a.weight", "layers.3.ffn.w_up_a.bias", "layers.3.ffn.w_up_b.weight", "layers.3.ffn.w_up_b.bias", "layers.3.ffn.w_down.weight", "layers.3.ffn.w_down.bias", "layers.3.norm1.weight", "layers.3.norm1.bias", "layers.3.norm2.weight", "layers.3.norm2.bias", "fc.weight", "fc.bias".
[rank0]: 	Unexpected key(s) in state_dict: "module.embeddings.weight", "module.layers.0.attention.rope.rope_sin", "module.layers.0.attention.rope.rope_cos", "module.layers.0.attention.w_q.weight", "module.layers.0.attention.w_q.bias", "module.layers.0.attention.w_k.weight", "module.layers.0.attention.w_k.bias", "module.layers.0.attention.w_v.weight", "module.layers.0.attention.w_v.bias", "module.layers.0.attention.w_o.weight", "module.layers.0.attention.w_o.bias", "module.layers.0.ffn.w_up_a.weight", "module.layers.0.ffn.w_up_a.bias", "module.layers.0.ffn.w_up_b.weight", "module.layers.0.ffn.w_up_b.bias", "module.layers.0.ffn.w_down.weight", "module.layers.0.ffn.w_down.bias", "module.layers.0.norm1.weight", "module.layers.0.norm1.bias", "module.layers.0.norm2.weight", "module.layers.0.norm2.bias", "module.layers.1.attention.rope.rope_sin", "module.layers.1.attention.rope.rope_cos", "module.layers.1.attention.w_q.weight", "module.layers.1.attention.w_q.bias", "module.layers.1.attention.w_k.weight", "module.layers.1.attention.w_k.bias", "module.layers.1.attention.w_v.weight", "module.layers.1.attention.w_v.bias", "module.layers.1.attention.w_o.weight", "module.layers.1.attention.w_o.bias", "module.layers.1.ffn.w_up_a.weight", "module.layers.1.ffn.w_up_a.bias", "module.layers.1.ffn.w_up_b.weight", "module.layers.1.ffn.w_up_b.bias", "module.layers.1.ffn.w_down.weight", "module.layers.1.ffn.w_down.bias", "module.layers.1.norm1.weight", "module.layers.1.norm1.bias", "module.layers.1.norm2.weight", "module.layers.1.norm2.bias", "module.layers.2.attention.rope.rope_sin", "module.layers.2.attention.rope.rope_cos", "module.layers.2.attention.w_q.weight", "module.layers.2.attention.w_q.bias", "module.layers.2.attention.w_k.weight", "module.layers.2.attention.w_k.bias", "module.layers.2.attention.w_v.weight", "module.layers.2.attention.w_v.bias", "module.layers.2.attention.w_o.weight", "module.layers.2.attention.w_o.bias", "module.layers.2.ffn.w_up_a.weight", "module.layers.2.ffn.w_up_a.bias", "module.layers.2.ffn.w_up_b.weight", "module.layers.2.ffn.w_up_b.bias", "module.layers.2.ffn.w_down.weight", "module.layers.2.ffn.w_down.bias", "module.layers.2.norm1.weight", "module.layers.2.norm1.bias", "module.layers.2.norm2.weight", "module.layers.2.norm2.bias", "module.layers.3.attention.rope.rope_sin", "module.layers.3.attention.rope.rope_cos", "module.layers.3.attention.w_q.weight", "module.layers.3.attention.w_q.bias", "module.layers.3.attention.w_k.weight", "module.layers.3.attention.w_k.bias", "module.layers.3.attention.w_v.weight", "module.layers.3.attention.w_v.bias", "module.layers.3.attention.w_o.weight", "module.layers.3.attention.w_o.bias", "module.layers.3.ffn.w_up_a.weight", "module.layers.3.ffn.w_up_a.bias", "module.layers.3.ffn.w_up_b.weight", "module.layers.3.ffn.w_up_b.bias", "module.layers.3.ffn.w_down.weight", "module.layers.3.ffn.w_down.bias", "module.layers.3.norm1.weight", "module.layers.3.norm1.bias", "module.layers.3.norm2.weight", "module.layers.3.norm2.bias", "module.fc.weight", "module.fc.bias".
